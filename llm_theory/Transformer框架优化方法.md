# Transformer/LM系列优化方法

## 1 复杂度计算

### 1.1 **自注意力机制的复杂度**

#### 1.1.1 计算流程

1. 将输入序列 $ X \in \mathbb{R}^{n \times d} $ 通过线性变换得到查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$，其中 $n$ 是序列长度，$d$ 是隐藏维度。
2. 计算注意力得分：$ A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) $，其中 $ A \in \mathbb{R}^{n \times n}\ $
3. 计算输出：$ O = AV $，其中 $ O \in \mathbb{R}^{n \times d} $

#### 1.1.2 时间复杂度

1. $Q,K,V,O$的线性变换：计算 $n \times d$ 矩阵与 $d \times d$ 矩阵的乘积，复杂度为 $O(nd^2)$

2. 矩阵乘法 $QK^T$：计算 $n \times d\ $矩阵与 $d \times n$ 矩阵的乘积，复杂度为 $O(n^2d)$ 

3. Softmax 操作：对 $n \times n$ 的注意力矩阵逐行应用 softmax，复杂度为 $ O(n^2)\ $

4. 矩阵乘法 $AV$：计算 $n \times n$ 矩阵与 $ n \times d\ $ 矩阵的乘积，复杂度为 $ O(n^2d)$

总时间复杂度 $ O(n^2d + nd^2) $，自注意力机制的**标志性瓶颈**是**注意力矩阵的计算与存储**，在实际应用中，$n$通常远大于$d$（如$n=10000$，$d=768$）

#### 1.1.3 空间复杂度

1. 存储 $Q, K, V, O$ 线性映射的参数矩阵：每个矩阵大小为 $d \times d$ ，总空间为 $O(4d^2) = O(d^2)$

2. 存储 $Q, K, V, O$：每个矩阵大小为 $n \times d $，总空间为 $ O(4nd) = O(nd)$

3. 存储注意力矩阵 A：大小为 $ n \times n$，空间为 $O(n^2)$

4. 其他临时变量：如中间结果等，通常为 $O(nd)$

总空间复杂度 $O(n^2 + d^2)$

### 1.2 前馈神经网络的复杂度

### 1.2.1 计算流程

前馈网络通常由两个线性层和一个激活函数组成：

1. 第一个线性层：$Y = \text{ReLU}(XW_1 + b_1)$，其中 $W_1 \in \mathbb{R}^{d \times 4d}$（通常中间维度为 $4d$）
2. 第二个线性层：$O = YW_2 + b_2$，其中 $W_2 \in \mathbb{R}^{4d \times d}$

### 1.2.2 时间复杂度

1. 第一个线性层：计算 $n \times d$ 矩阵与 $ d \times 4d\ $ 矩阵的乘积，复杂度为 $ O(n \cdot d \cdot 4d) = O(4nd^2) = O(nd^2)\ $

2. 第二个线性层：计算 $n \times 4d $ 矩阵与 $ 4d \times d $ 矩阵的乘积，复杂度为 $ O(n \cdot 4d \cdot d) = O(4nd^2) = O(nd^2) $

总时间复杂度：$ O(nd^2) $

### 1.2.3 空间复杂度

1. 存储权重矩阵 $ W_1$ 和 $W_2 $：分别为 $ d \times 4d$ 和 $ 4d \times d$，总空间为 $ O(4d^2 + 4d^2) = O(d^2)\ $

2. 存储输入 $ X$ 和输出 $ O$：每个大小为 $ n \times d$，总空间为 $ O(2nd) = O(nd)$

总空间复杂度：$ O(nd + d^2)$

## 1.3 完整 Transformer 的复杂度

对于具有 L 层的 Transformer 模型：

- **总时间复杂度**：自注意力 $ O(Ln^2d + Lnd^2) $ + 前馈网络 $ O(Lnd^2) $
- **总空间复杂度**：自注意力 $ O(L(n^2 + d^2))$ + 前馈网络 $ O(L(nd + d^2)) $

短序列任务 $ d $ 主导，长序列任务 $ n $ 主导

# 2 **注意力机制优化**

## 2.1 Flash Attention

**核心思想**：通过分块计算（tiling）和重排序算法，优化内存访问模式，减少高带宽内存（HBM）与计算单元之间的数据传输。

**技术细节**：

- 将输入张量 $ Q, K, V $ 划分为小的块（tiles），逐块计算注意力得分并累积结果。
- 避免显式存储完整的注意力矩阵，直接在计算过程中更新输出。
- 采用 I/O 感知算法，使数据访问模式与 GPU 内存层次结构匹配。

**复杂度分析**：

- **时间复杂度**：$ O(n^2d + nd^2) $（与标准注意力相同，但常数项显著减小）
- **空间复杂度**：$ O(nd) $（减少了 $ O(n^2)\ $ 的注意力矩阵存储）

**为什么有效**：通过算法与硬件协同设计，大幅降低内存访问开销，提升计算效率。实验表明，FlashAttention 可将注意力计算速度提高 3-5 倍，内存占用减少约 70%

## 2.2 线性注意力

![](https://picx.zhimg.com/v2-170155ce4bec61077b13936a73bccfef_r.jpg)

**核心思想**：用线性核函数替代标准注意力中的 softmax 操作，将复杂度从 $ O(n^2d)$ 降至 $ O(nd^2) $ ，这样就和序列长度 $ n$线性相关了

**技术细节**：

- 标准注意力：$ O = \text{softmax}(QK^T/\sqrt{d})V $
- 线性注意力：$ O = \frac{Q(K^TV)}{\sum(K^T)} $，其中 $ Q $ 和 $ K $ 通过线性核函数（如 ReLU、exp）变换

**复杂度分析**：

- **时间复杂度**：$ O(nd^2) $，计算 $ K^TV $ 和 $ Q(K^TV $）
- **空间复杂度**：$ O(nd) $（只需存储 $ Q, K, V $ 和中间结果 $ K^TV $）

**为什么有效**：通过数学变换避免直接计算 $ QK^T $ 矩阵，将二次方复杂度降为线性。但可能牺牲一定的建模能力（如长距离依赖捕捉）

## 2.3 稀疏注意力

**核心思想**：只计算部分关键位置之间的注意力，减少计算量。

**技术实现**：

- **固定模式稀疏**：如局部窗口注意力（每个 token 只关注周围 k 个 token）、扩张注意力（每隔 d 个位置计算一次）。
- **动态稀疏**：根据输入内容自适应选择关注的位置（如使用聚类或重要性采样）。

**复杂度分析**：

- **时间复杂度**：$O(nkd)$（k 为每个 token 关注的位置数，$k \ll n$）
- **空间复杂度**：$O(nk)$（存储稀疏注意力矩阵）

**为什么有效**：通过限制注意力计算范围，将复杂度从二次方降至近线性。适用于序列中有局部结构或关键信息稀疏分布的场景。

## 2.4 因果稀疏注意力（如 Longformer）

**核心思想**：结合局部注意力和全局注意力，在保持因果性的同时减少计算量。

**技术细节**：

- 每个 token 关注：
  - 局部窗口内的所有 token（如左右各 w 个位置）。
  - 少量全局 token（如特殊标记或关键位置）。

**复杂度分析**：

- **时间复杂度**：$ O(nwd + ngd) $（w 为局部窗口大小，g 为全局 token 数）
- **空间复杂度**：$ O(n(w+g)) $

**为什么有效**：在长序列任务中，大部分依赖关系是局部的，全局注意力仅用于捕获关键远程依赖，平衡了效率和表达能力

# 3 **内存优化技术**

## 3.1  KV Cache（键值缓存）

**核心思想**：在**自回归**生成中，缓存历史 token 的 Key 和 Value 矩阵，避免重复计算。

**技术细节**：

- 在生成第 t 个 token 时，仅计算当前 token 的 $Q_t$，并复用之前所有 token 的 $K_{1:t-1}$ 和 $V_{1:t-1}$。
- 注意力计算简化为：$A_t = \text{softmax}(Q_tK_{1:t-1}^T)V_{1:t-1}$。

**复杂度分析**：

- **时间复杂度**：每步生成 $O(nd)$（标准注意力为 $O(n^2d)$）
- **空间复杂度**：$O(nd)$（存储所有历史 K 和 V）

**为什么有效**：将每步生成的时间复杂度从二次方降至线性，特别适合长文本生成任务（如对话、摘要）。

## 3.2 Recompute（激活重计算 / 检查点技术）

**核心思想**：在反向传播时重新计算部分中间激活值，以减少正向传播时的内存占用。

**技术细节**：

- 正向传播时，只存储部分关键激活值（检查点），其余中间结果丢弃。
- 反向传播时，根据检查点重新计算丢弃的激活值。

**复杂度分析**：

- **时间复杂度**：增加约 20-30% 的计算量（重新计算激活值）
- **空间复杂度**：减少约 50-70% 的内存占用（无需存储所有中间激活值）

**为什么有效**：用计算时间换取内存空间，在显存受限的情况下训练更大模型

# 4 **架构优化**

## 4.1 混合专家模型（MoE, Mixture of Experts）

**核心思想**：将模型参数划分为多个 “专家”，每个输入 token 仅激活部分专家，减少计算量。

**技术细节**：

- 门控网络（Gating Network）为每个 token 选择 k 个专家。
- 仅计算被选中专家的输出，然后加权组合。

**复杂度分析**：

- **时间复杂度**：$O(n(d^2/k + d \cdot \text{专家数}))$（k 为每个 token 激活的专家数）
- **空间复杂度**：$O(d^2 \cdot \text{专家数})$（存储所有专家参数）

**为什么有效**：在保持模型容量的同时，减少每个 token 的实际计算量。适合大规模模型（如万亿参数）。

## 4.2 滑动窗口注意力（Sliding Window Attention）

**核心思想**：每个 token 仅关注其前后固定窗口内的 token，避免全局计算。

**技术细节**：

- 对于序列长度为 n 的输入，每个 token 关注窗口大小为 w 的上下文。
- 窗口可重叠或不重叠，支持因果或双向注意力。

**复杂度分析**：

- **时间复杂度**：$O(nwd)$（w 为窗口大小，通常 $w \ll n$）
- **空间复杂度**：$O(nw)$

**为什么有效**：将全局注意力的二次方复杂度降至线性，特别适合超长序列（如文档、视频）
